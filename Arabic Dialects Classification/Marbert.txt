#- coding: utf-8 -*-
"""marabert.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FFeC35kHIybFm7-gbxu3HS_Ntnogb7Ar
"""



import torch
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer, AutoModelForSequenceClassification,BertConfig,BertModel
from torch.utils.data import DataLoader, TensorDataset
from datasets import load_dataset
from sklearn.preprocessing import OneHotEncoder
import torch.nn.functional as F

from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score

import torch.optim as optim
import torch.nn as nn
import torch.nn as nn
class MixModel(nn.Module):
    def __init__(self,pre_trained='bert-base-uncased'):
        super().__init__()
        
        #config = AutoConfig.from_pretrained("bert-base-cased", output_hidden_states=True)
        config = BertConfig.from_pretrained('UBC-NLP/MARBERT', output_hidden_states=True)
        self.bert =  BertModel.from_pretrained('UBC-NLP/MARBERT',config=config)
        self.hidden_size = self.bert.config.hidden_size
     
        
       
        self.clf = nn.Linear(self.hidden_size,18)
      
           
    
    def forward(self, inputs, mask, labels):
        cls_hs = self.bert(input_ids=inputs, attention_mask=mask, return_dict=True)

        xx = self.clf(cls_hs['last_hidden_state'][:, 0, :])  # Use the [CLS] token representatio
        
        
       
              
     
        
        return xx
  
  
        
              
train_data = load_dataset('csv', data_files=r"/home2/nour/sherouk/nadi paper/task12023_processing_train.csv",split='train')
val_data= load_dataset('csv', data_files=r"/home2/nour/sherouk/nadi paper/task12023_processing_dev.csv",split='train')
test_data= load_dataset('csv', data_files=r"/home2/nour/sherouk/nadi paper/task12023_processing_test.csv",split='train')

# Load the tokenizer and model
tokenizer = AutoTokenizer.from_pretrained("UBC-NLP/MARBERT")

marbert_model=MixModel()



# Tokenize and encode the training data
train_encodings = tokenizer(list(train_data['new_content']), truncation=True, padding=True, max_length=128, return_tensors="pt")
input_ids = train_encodings["input_ids"]
attention_masks = train_encodings["attention_mask"]

# Tokenize and encode the validation data
val_encodings = tokenizer(list(val_data['new_content']), truncation=True, padding=True, max_length=128, return_tensors="pt")
d_input_ids = val_encodings["input_ids"]
d_attention_mask = val_encodings["attention_mask"]

test_encodings = tokenizer(list(test_data['new_content']), truncation=True, padding=True, max_length=128, return_tensors="pt")
t_input_ids = test_encodings["input_ids"]
t_attention_mask = test_encodings["attention_mask"]

# Convert labels to tensor
train_labels = torch.tensor(train_data['symbol'])
val_labels = torch.tensor(val_data['symbol'])

# Create a DataLoader for training
train_dataset = TensorDataset(input_ids, attention_masks, train_labels)
train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)

val_dataset = TensorDataset(input_ids, attention_masks, train_labels)
val_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)

# Define loss function and optimizer

criterion = nn.CrossEntropyLoss(reduction='mean')
optimizer = optim.AdamW(marbert_model.parameters(), lr=1e-5)
marbert_model.load_state_dict(torch.load("/home2/nour/sherouk/nadi paper/marbert simple1_epoc_2.pt"))


with torch.no_grad():        
        val_outputs = marbert_model(d_input_ids, d_attention_mask,val_labels)
        #val_logits = val_outputs.logit
        print(val_outputs.shape)
        reshaped_val_outputs = val_outputs.reshape(1800, 18)
        print(reshaped_val_outputs.shape)
        val_pred = reshaped_val_outputs.argmax(dim=1)
        print(val_pred)
        #val_pred = val_logits
        #print(val_pred)
        #print(val_logits)
        
        val_accuracy = (val_pred == val_labels).float().mean()
        
print(f"Validation Accuracy: {val_accuracy:.4f}")
print (classification_report(val_pred, val_labels))
print (confusion_matrix(val_pred, val_labels))
print (f1_score(val_pred, val_labels,average='macro'))

print (precision_score(val_pred, val_labels,average='macro'))
print (recall_score(val_pred, val_labels,average='macro'))  
#t=np.argmax(val_pred , axis=1)
df = pd.DataFrame({'col':val_pred})
id2label = {    0: 'Yemen',
    1: 'Lebanon',
    2: 'Algeria',
    3: 'Iraq',
    4: 'Egypt',
    5: 'Saudi_Arabia',
    6: 'Libya',
    7: 'Oman',
    8: 'Palestine',
    9: 'Syria',
    10: 'Bahrain',

    11: 'Tunisia',
    12: 'Qatar',
    13: 'Jordan',
    14: 'Sudan',
    15: 'Kuwait',
    16: 'Morocco',

    17: 'UAE'
}
df['country_labels'] = df['col'].replace(id2label)
with open("/home2/nour/sherouk/nadi paper/ISL-AAST_subtask1_dev_6.txt", "w+") as f:
  data = f.read()
  f.write("\n".join(df['country_labels']))
np.save('/home2/nour/sherouk/nadi paper/dev6_predictions.npy', val_pred)
np.save('/home2/nour/sherouk/nadi paper/dev6_predictions_probablites.npy', reshaped_val_outputs)
     
        
with torch.no_grad():        
        test_outputs = marbert_model(t_input_ids, t_attention_mask,val_labels)
        #val_logits = val_outputs.logit
        print(test_outputs .shape)
        reshaped_test_outputs = test_outputs.reshape(3600, 18)
        print(reshaped_test_outputs.shape)
        test_pred = reshaped_test_outputs.argmax(dim=1)
        print(test_pred)
        #val_pred = val_logits
        #print(val_pred)
        #print(val_logits)
        
       #test_accuracy = (test_pred == val_labels).float().mean()        
     
    


np.save('/home2/nour/sherouk/nadi paper/test6_predictions_probablites.npy', reshaped_test_outputs)
df2 = pd.DataFrame({'col':test_pred})
id2label = {    0: 'Yemen',
    1: 'Lebanon',
    2: 'Algeria',
    3: 'Iraq',
    4: 'Egypt',
    5: 'Saudi_Arabia',
    6: 'Libya',
    7: 'Oman',
    8: 'Palestine',
    9: 'Syria',
    10: 'Bahrain',
    11: 'Tunisia',
    12: 'Qatar',
    13: 'Jordan',
    14: 'Sudan',
    15: 'Kuwait',
    16: 'Morocco',
    17: 'UAE'
}
np.save('/home2/nour/sherouk/nadi paper/test6_predictions.npy', test_pred)
df2['country_labels'] = df2['col'].replace(id2label)
with open("/home2/nour/sherouk/nadi paper/ISL-AAST_subtask1_dev_6.txt", "w+") as f:
  data = f.read()
  f.write("\n".join(df2['country_labels']))  
     
    
    

