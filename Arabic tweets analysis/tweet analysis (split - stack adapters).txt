# -*- coding: utf-8 -*-
"""Copy of final TASK2NEW _Nadi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LHN9liv4YBN1WmVkIAubuNifJLVUBK8u
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install -U adapter-transformers

from google.colab import drive
drive.mount('/content/drive')

!pip install -U adapter-transformers
!pip install datasets

import transformers.adapters.composition as ac
from datasets import load_dataset
from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer, AutoModelWithHeads, AutoConfig, AdapterConfig
from transformers import TrainingArguments, Trainer, EvalPrediction
import numpy as np
import pandas as pd

# Load datasets
traindataset1 = load_dataset('csv', data_files='/content/drive/My Drive/NADI2022/task2_processing_train_emoji2.csv', split='train')
traindataset2 = load_dataset('csv', data_files='/content/drive/My Drive/NADI2022/preprocess_Train2_task2_10746.csv', split='train')
traindataset3 = load_dataset('csv', data_files='/content/drive/My Drive/NADI2022/preprocess_Train2_task2_6123.csv', split='train')

# Merge datasets
df1 = pd.read_csv('/content/drive/My Drive/NADI2022/task2_processing_train_emoji2.csv')
df2 = pd.read_csv('/content/drive/My Drive/NADI2022/task2_processing_collection2.csv')
train, test = train_test_split(df2, test_size=0.7)

data = df1.append(train, ignore_index=True)

# Tokenization and encoding functions...
tokenizer = AutoTokenizer.from_pretrained("UBC-NLP/MARBERT")

def encode(examples):
    return tokenizer(str(examples['new_content']), truncation=True, padding='max_length', max_length=64, pad_to_max_length=True)

# Apply tokenization and encoding functions
traindataset1 = traindataset1.map(encode)
traindataset2 = traindataset2.map(encode)
traindataset3 = traindataset3.map(encode)

# Configure and train models with Split, Fusion, and Stack adapters
config1 = AutoConfig.from_pretrained("UBC-NLP/MARBERT", num_labels=3)
model1 = AutoModelWithHeads.from_pretrained("UBC-NLP/MARBERT", config=config1)
model1.add_adapter("cls-arabic_marabert")
model1.add_classification_head("cls-arabic_marabert", num_labels=3, id2label={0: 'neg', 1: 'neut', 2: 'pos'})
model1.train_adapter("cls-arabic_marabert")

config2 = AutoConfig.from_pretrained("alger-ia/dziribert")
model2 = AutoModelWithHeads.from_pretrained("alger-ia/dziribert", config=config2)
model2.add_adapter("cls-arabic_dizir")
model2.add_classification_head("cls-arabic_dizir", num_labels=3, id2label={0: 'neg', 1: 'neut', 2: 'pos'})
model2.train_adapter("cls-arabic_dizir")

config3 = AutoConfig.from_pretrained("qarib/bert-base-qarib")
model3 = AutoModelWithHeads.from_pretrained("qarib/bert-base-qarib", config=config3)
model3.add_adapter("cls-arabic_quarib")
model3.add_classification_head("cls-arabic_quarib", num_labels=3, id2label={0: 'neg', 1: 'neut', 2: 'pos'})
model3.train_adapter("cls-arabic_quarib")

# Apply Split, Fusion, and Stack adapters
model1.add_adapter("cls-arabic_marabert1")
model1.add_adapter("cls-arabic_marabert2")

model1.add_adapter("cls-arabic_marabert3")
model1.add_adapter("cls-arabic_marabert4")

model1.active_adapters = ac.Split("cls-arabic_marabert1", "cls-arabic_marabert2", split_index=32)
model1.train_adapter(ac.Split("cls-arabic_marabert1", "cls-arabic_marabert2", split_index=32))

model2.add_adapter("cls-arabic_dizir1")
model2.add_adapter("cls-arabic_dizir2")

model2.active_adapters = ac.Stack("cls-arabic_dizir1", "cls-arabic_dizir2")
model2.train_adapter(ac.Stack("cls-arabic_dizir1", "cls-arabic_dizir2"))

model3.add_adapter("cls-arabic_quarib1")
model3.add_adapter("cls-arabic_quarib2")

model3.active_adapters = ac.ParallelLand("cls-arabic_quarib1", "cls-arabic_quarib2")
model3.train_adapter(ac.ParallelLand("cls-arabic_quarib1", "cls-arabic_quarib2"))

# Define encode functions...
def encode(examples):
    return tokenizer(str(examples['new_content']), truncation=True, padding='max_length', max_length=64, pad_to_max_length=True)

# Apply tokenization and encoding functions
traindataset1 = traindataset1.map(encode)
traindataset2 = traindataset2.map(encode)
traindataset3 = traindataset3.map(encode)


config1 = AutoConfig.from_pretrained("UBC-NLP/MARBERT", num_labels=3)
model1 = AutoModelWithHeads.from_pretrained("UBC-NLP/MARBERT", config=config1)
model1.add_adapter("cls-arabic_marabert")
model1.add_classification_head("cls-arabic_marabert", num_labels=3, id2label={0: 'neg', 1: 'neut', 2: 'pos'})
model1.train_adapter("cls-arabic_marabert")

config2 = AutoConfig.from_pretrained("dizir")
model2 = AutoModelWithHeads.from_pretrained("dizir", config=config2)
model2.add_adapter("cls-arabic_dizir")
model2.add_classification_head("cls-arabic_dizir", num_labels=3, id2label={0: 'neg', 1: 'neut', 2: 'pos'})
model2.train_adapter("cls-arabic_dizir")

config3 = AutoConfig.from_pretrained("qarib/bert-base-qarib")
model3 = AutoModelWithHeads.from_pretrained("qarib/bert-base-qarib", config=config3)
model3.add_adapter("cls-arabic_quarib")
model3.add_classification_head("cls-arabic_quarib", num_labels=3, id2label={0: 'neg', 1: 'neut', 2: 'pos'})
model3.train_adapter("cls-arabic_quarib")

# Apply Split, Fusion, and Stack adapters
model1.add_adapter("cls-arabic_marabert1")
model1.add_adapter("cls-arabic_marabert2")
model1.add_adapter("cls-arabic_marabert3")
model1.add_adapter("cls-arabic_marabert4")

model1.active_adapters = ac.Split("cls-arabic_marabert1", "cls-arabic_marabert2", split_index=32)
model1.train_adapter(ac.Split("cls-arabic_marabert1", "cls-arabic_marabert2", split_index=32))

model2.add_adapter("cls-arabic_dizir1")
model2.add_adapter("cls-arabic_dizir2")

model2.active_adapters = ac.Stack("cls-arabic_dizir1", "cls-arabic_dizir2")
model2.train_adapter(ac.Stack("cls-arabic_dizir1", "cls-arabic_dizir2"))

model3.add_adapter("cls-arabic_quarib1")
model3.add_adapter("cls-arabic_quarib2")

model3.active_adapters = ac.ParallelLand("cls-arabic_quarib1", "cls-arabic_quarib2")
model3.train_adapter(ac.ParallelLand("cls-arabic_quarib1", "cls-arabic_quarib2"))

# Continue with training arguments, trainers, and evaluations as before...

# Training Arguments
training_args1 = TrainingArguments(
    learning_rate=2e-5,
    num_train_epochs=8,
    per_device_train_batch_size=128,
    per_device_eval_batch_size=128,
    logging_steps=200,
    do_predict=True,
    output_dir="/content/drive/My Drive/NADI2022/task2_2",
    overwrite_output_dir=True,
    remove_unused_columns=True,
)

# Trainer for model1
trainer1 = Trainer(
    model=model1,
    args=training_args1,
    train_dataset=traindataset1,
    eval_dataset=DEVdataset1,
    compute_metrics=compute_accuracy,
)

trainer1.train()
trainer1.evaluate()

# Continue similarly for model2 and model3...

# Save the models
model1.save_adapter("/content/drive/My Drive/NADI2022/task2_final1", "cls-arabic_language1")
model2.save_adapter("/content/drive/My Drive/NADI2022/task2_final2", "cls-arabic_language2")
model3.save_adapter("/content/drive/My Drive/NADI2022/task2_final3", "cls-arabic_language3")
