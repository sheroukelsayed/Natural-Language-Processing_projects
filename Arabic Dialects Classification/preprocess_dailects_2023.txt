Sure, I've organized and cleaned your code. I've removed duplicate imports, consolidated similar libraries, and added some comments for clarity. Here's the cleaned code:

```python
# -*- coding: utf-8 -*-
"""Preprocess_task1_2023.ipynb

Automatically generated by Colaboratory.

Original file is located at
https://colab.research.google.com/drive/1YNW7iwxp6WysJZeOL3eWQhiior9wS_Q_
"""

# Imports
from google.colab import drive
import tensorflow as tf
import tensorflow_hub as hub
import pandas as pd
from sklearn.model_selection import train_test_split
import numpy as np
import re
import nltk
from nltk.corpus import stopwords
import keras
from tqdm import tqdm
from keras.models import Model
import keras.backend as K
from sklearn.metrics import confusion_matrix, f1_score, classification_report
import matplotlib.pyplot as plt
from keras.callbacks import ModelCheckpoint
from keras.models import load_model
from sklearn.utils import shuffle
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.stem import PorterStemmer
import os
import csv
from PIL import Image
import emoji

# Download NLTK resources
nltk.download('wordnet')
nltk.download('punkt')
nltk.download('stopwords')

# Install emoji library
!pip install emoji

# Mount Google Drive
drive.mount('/content/drive')

# Install ktrain
!pip3 install ktrain

# Function to remove diacritics from text
def remove_diacritics(text):
    arabic_diacritics = re.compile(""" ّ    | # Tashdid
                             َ    | # Fatha
                             ً    | # Tanwin Fath
                             ُ    | # Damma
                             ٌ    | # Tanwin Damm
                             ِ    | # Kasra
                             ٍ    | # Tanwin Kasr
                             ْ    | # Sukun
                             ـ     # Tatwil/Kashida
                         """, re.VERBOSE)
    text = re.sub(arabic_diacritics, '', str(text))
    return text

# Function to remove specific words from text
def remove_words(text):
    # List of words to remove
    words_to_remove = ['USER', 'URL', 'User', 'url', 'Url', 'NUM', 'الله']
    pattern = r'\b(?:' + '|'.join(re.escape(word) for word in words_to_remove) + r')\b'
    cleaned_text = re.sub(pattern, '', text, flags=re.IGNORECASE)
    cleaned_text = re.sub(r'#\w+', '', cleaned_text)
    cleaned_text = re.sub(r'@\w+', '', text)
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text)
    return cleaned_text.strip()

# Function to remove emojis from text
def remove_emojis(text):
    emoji_pattern = re.compile(
        pattern="["
        u"\U0001F600-\U0001F64F"  # Emoticons
        u"\U0001F300-\U0001F5FF"  # Symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # Transport & map symbols
        u"\U0001F700-\U0001F77F"  # Alchemical symbols
        u"\U0001F780-\U0001F7FF"  # Geometric shapes extended
        u"\U0001F800-\U0001F8FF"  # Supplemental symbols and pictographs
        u"\U0001F900-\U0001F9FF"  # Supplemental symbols and ideographs
        u"\U0001FA00-\U0001FA6F"  # Chess symbols
        u"\U0001FA70-\U0001FAFF"  # Symbols and pictographs extended-A
        u"\U00002700-\U000027BF"  # Dingbats
        "]+",
        flags=re.UNICODE,
    )
    return emoji_pattern.sub(r"", text)

# Function to remove elongated characters
def remove_elongates(text):
    pattern = r'(.)\1+'
    clean_text = re.sub(pattern, r'\1', text)
    return clean_text

# Function to clean text
def clean_text(text):
    text = "".join([word for word in text if word not in punctuations_list])
    text = remove_emojis(text)
    text = remove_diacritics(text)
    text = remove_elongates(text)
    text = remove_words(text)
    tokens = word_tokenize(text)
    return tokens

# Define Arabic stopwords
arabic_stopwords = list(map(str.strip, stopwords.open('arabic')))

# Define Arabic punctuations
arabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:"؟.,'{}~¦+|!”…“–ـ'''
english_punctuations = string.punctuation
punctuations_list = arabic_punctuations + english_punctuations

# Load data
train = pd.read_csv('/content/drive/My Drive/NADI 2023/NADI/Subtask1/NADI2023_Subtask1_TRAIN.tsv', sep='\t')
DEV = pd.read_csv('/content/drive/My Drive/NADI 2023/NADI/Subtask1/NADI2023_Subtask1_DEV.tsv', sep='\t')
test = pd.read_csv('/content/drive/My Drive/NADI 2023/NADI/Subtask1/NADI2023_Subtask1_TEST_Unlabeled.tsv', sep='\t')

# Sample DataFrame
data = {'Text': ['apple apple apple banana name repeat', 'orange orange orange hi welcome', 'grape grape grape grape me', 'kiwi kiwi kiwi']}
df1 = pd.DataFrame(data)

# Function to remove repeated words
def remove_repeated_words(data_frame, column_name, threshold):
    cleaned_column = data_frame[column_name].apply(lambda x: ' '.join([word for word in x.split() if x.count(word) <= threshold]))
    data_frame[column_name] = cleaned_column
    return data_frame

# Specify the column name and the threshold
column_to_check = 'tweet'
threshold_count = 300

# Remove rows with repeated words more than the threshold
cleaned_df = remove_repeated_words(df1, column_to_check, threshold_count)

# Initialize stemmers and lemmatizer
stemmer = ISRIStemmer()
lemmatizer = WordNetLemmatizer()

# Sample Arabic words
words = ['كتبت', 'كتابي', 'كاتب', 'الكتب']

# Stemming
stemmed_words = [stemmer.stem(word) for word in words]

# Lemmatization
lemmatized_words = [lemmatizer.lemmatize(word) for word in words]

print("Original Words:", words)
print("Stemmed Words:", stemmed_words)
print("Lemmatized Words:", lemmatized_words)

# Process text using NLTK
def process_text(text):
    stemmer = nltk.IS

RIStemmer()
    word_list = nltk.word_tokenize(text)
    return ' '.join(word_list)

# Apply text processing to sample text
texts = process_text(" they are eating a lot انتم تفوزون كتبت كتاب يكتب و تكسبون كثيرااااا")
print(texts)

# Specify the column name and the threshold
column_to_check = 'Text'
threshold_count = 300  # Adjust the threshold as needed

# Find repeated words more than the threshold
repeated_words = find_repeated_words(train, column_to_check, threshold_count)

print("Repeated Words and Their Counts:")
for word, count in repeated_words.items():
    print(f"{word}: {count}")

# Read additional datasets
dt2018 = pd.read_csv('/content/drive/My Drive/NADI 2023/NADI/Subtask1/MADAR-2018.tsv', sep='\t')
dt2021 = pd.read_csv('/content/drive/My Drive/NADI 2023/NADI/Subtask1/NADI2020-TWT.tsv', sep='\t')
dt2022 = pd.read_csv('/content/drive/My Drive/NADI 2023/NADI/Subtask1/NADI2021-TWT.tsv', sep='\t')

# Concatenate datasets
result = pd.concat([df2021, df2020, train1])

# Rename columns
result = result.rename(columns={'#1 tweet_ID': 'id', '#2 tweet_content': 'content', '#3 country_label': 'label'}, inplace=False)

# Save processed datasets
result.to_csv('/content/drive/My Drive/NADI 2023/NADI/Task1/task1_3datasets.csv', index=False)

# Class weights calculation
classes = [0, 1, 2]
class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(train1['symbol']), y=train1['symbol'])
class_weights = dict(enumerate(class_weights))

print(class_weights)

# Save processed datasets
train1.to_csv('/content/drive/My Drive/NADI 2023/NADI/Task1/task12023_processing_train.csv', index=False)
DEV1.to_csv('/content/drive/My Drive/NADI 2023/NADI/Task1/task12023_processing_dev.csv', index=False)
test1.to_csv('/content/drive/My Drive/NADI 2023/NADI/Task1/task12023_processing_test.csv', index=False)
```