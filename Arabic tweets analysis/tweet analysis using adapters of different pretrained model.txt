# -*- coding: utf-8 -*-
"""final TASK2NEW _Nadi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bGv6p8aDAO_5hAdAw_PQoRMjLFI9CiID
"""

# Import necessary libraries
from google.colab import drive
from transformers import AutoTokenizer, AutoModelWithHeads, TrainingArguments, AdapterTrainer, EvalPrediction, AdapterConfig
from datasets import load_dataset
import pandas as pd
import numpy as np

# Mount Google Drive
drive.mount('/content/drive')

# Install required libraries
!pip install -U adapter-transformers
!pip install datasets

# Load datasets
traindataset1 = load_dataset('csv', data_files='/content/drive//My Drive/NADI2022/task2_processing_train_emoji2.csv', split='train')
traindataset2 = load_dataset('csv', data_files='/content/drive//My Drive/NADI2022/preprocess_Train2_task2_10746.csv', split='train')
traindataset3 = load_dataset('csv', data_files='/content/drive//My Drive/NADI2022/preprocess_Train2_task2_6123.csv', split='train')
DEVdataset = load_dataset('csv', data_files='/content/drive//My Drive/NADI2022/task2_processing_dev_emoji2.csv', split='train')

# Read CSV files into Pandas DataFrames
df1 = pd.read_csv('/content/drive//My Drive/NADI2022/task2_processing_train_emoji2.csv')
df2 = pd.read_csv('/content/drive//My Drive/NADI2022/task2_processing_collection2.csv')

# Split training data
train, _ = train_test_split(df2, test_size=0.7)
train2, _ = train_test_split(df2, test_size=0.6)

# Combine dataframes
data = df1.append(train, ignore_index=True)

# Create datasets
dataset = Dataset.from_pandas(train)
dataset2 = Dataset.from_pandas(train2)
DEVdataset = load_dataset('csv', data_files='/content/drive//My Drive/NADI2022/task2_processing_dev_emoji2.csv', split='train')

# Initialize tokenizers
tokenizer = AutoTokenizer.from_pretrained("UBC-NLP/MARBERT")
tokenizer2 = AutoTokenizer.from_pretrained("UBC-NLP/MARBERTv2")
tokenizer3 = AutoTokenizer.from_pretrained("alger-ia/dziribert")
tokenizer4 = AutoTokenizer.from_pretrained("qarib/bert-base-qarib")

# Define encoding functions for tokenization
def encode(examples, tokenizer):
    return tokenizer(str(examples['new_content']), truncation=True, padding='max_length', max_length=64, pad_to_max_length=True)

# Map encoding functions to datasets
traindataset1 = traindataset1.map(lambda examples: {'labels': examples['symbol2']}, batched=True)
traindataset1 = traindataset1.map(encode, fn_kwargs={'tokenizer': tokenizer})
traindataset2 = traindataset2.map(lambda examples: {'labels': examples['symbol2']}, batched=True)
traindataset2 = traindataset2.map(encode, fn_kwargs={'tokenizer': tokenizer2})
traindataset3 = traindataset3.map(lambda examples: {'labels': examples['symbol2']}, batched=True)
traindataset3 = traindataset3.map(encode, fn_kwargs={'tokenizer': tokenizer3})
DEVdataset = DEVdataset.map(lambda examples: {'labels': examples['symbol2']}, batched=True)
DEVdataset = DEVdataset.map(encode, fn_kwargs={'tokenizer': tokenizer})
# Continue encoding functions for the remaining datasets and tokenizers
DEVdataset1 = DEVdataset.map(encode2, fn_kwargs={'tokenizer': tokenizer2})
DEVdataset2 = DEVdataset.map(encode3, fn_kwargs={'tokenizer': tokenizer3})
DEVdataset3 = DEVdataset.map(encode4, fn_kwargs={'tokenizer': tokenizer4})

# Define training arguments for AdapterTrainer
training_args1 = TrainingArguments(
    learning_rate=2e-5,
    num_train_epochs=5,
    per_device_train_batch_size=64,
    per_device_eval_batch_size=64,
    logging_steps=50,
    do_predict=True,
    output_dir="/content/drive//My Drive/NADI2022/task2_22",
    overwrite_output_dir=True,
    remove_unused_columns=True,
)

# Define a function to compute accuracy for AdapterTrainer
def compute_accuracy(p: EvalPrediction):
    preds = np.argmax(p.predictions, axis=1)
    return {"acc": (preds == p.label_ids).mean()}

# Initialize AdapterTrainer for model1
trainer1 = AdapterTrainer(
    model=model1,
    args=training_args1,
    train_dataset=dataset1,
    eval_dataset=DEVdataset1,
    compute_metrics=compute_accuracy,
)

# Train and evaluate the model
trainer1.train()
trainer1.evaluate()

# Save the trained adapter
model1.save_adapter("/content/drive//My Drive/NADI2022/task2_final1", "cls-arabic_language1")

# Continue similarly for model2, model3, and model4
# ...

# Combine predictions from different models
output1 = trainer1.predict(DEVdataset1)
output2 = trainer2.predict(DEVdataset1)
output3 = trainer3.predict(DEVdataset1)
output_test1 = trainer1.predict(testdataset1)
output_test2 = trainer2.predict(testdataset1)
output_test3 = trainer3.predict(testdataset1)

# Sum predictions for ensemble
ensemble_predictions = output1[0] + output2[0] + output3[0]

# Extract labels from predictions
ensemble_labels = np.argmax(ensemble_predictions, axis=1)

# Save ensemble predictions to a file
df_ensemble = pd.DataFrame({'col': ensemble_labels})
id2label = {0: 'neg', 1: 'neut', 2: 'pos'}
df_ensemble['country_labels'] = df_ensemble['col'].replace(id2label)
with open("/content/drive//My Drive/NADI2022/task2_ensemble.txt", "w+") as f:
    f.write("\n".join(df_ensemble['country_labels']))
